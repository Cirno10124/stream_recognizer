 # Whisper LoRA微调配置文件
# 请根据您的需求修改以下参数

# 模型配置
model:
  name_or_path: "openai/whisper-small"  # 可选: whisper-tiny, whisper-base, whisper-small, whisper-medium, whisper-large
  cache_dir: "./model_cache"            # 模型缓存目录
  use_fast_tokenizer: true
  model_revision: "main"
  use_auth_token: false

# 数据配置
data:
  train_file: "./data/train.json"         # 训练数据文件
  validation_file: "./data/validation.json"  # 验证数据文件
  test_file: null                         # 测试数据文件（可选）
  max_train_samples: null                 # 最大训练样本数（null表示使用全部）
  max_eval_samples: null                  # 最大验证样本数
  audio_column_name: "audio"              # 音频列名称
  text_column_name: "sentence"            # 文本列名称
  max_duration: 30.0                      # 最大音频长度（秒）
  min_duration: 1.0                       # 最小音频长度（秒）
  preprocessing_num_workers: 4            # 预处理工作线程数

# LoRA配置
lora:
  use_lora: true                          # 是否使用LoRA
  r: 8                                    # LoRA rank (4-64)，值越大参数越多
  alpha: 32                               # LoRA alpha，通常为r的2-4倍
  dropout: 0.1                            # LoRA dropout率
  target_modules:                         # 目标模块列表
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "out_proj"
    - "fc1"
    - "fc2"
  bias: "none"                            # bias类型: "none", "all", "lora_only"

# 训练配置
training:
  output_dir: "./output"                  # 输出目录
  overwrite_output_dir: true              # 是否覆盖输出目录
  
  # 训练轮数和批次
  num_train_epochs: 3                     # 训练轮数
  per_device_train_batch_size: 4          # 每设备训练批次大小
  per_device_eval_batch_size: 4           # 每设备验证批次大小
  gradient_accumulation_steps: 2          # 梯度累积步数
  
  # 学习率和优化器
  learning_rate: 5e-4                     # 学习率
  weight_decay: 0.01                      # 权重衰减
  warmup_steps: 500                       # 预热步数
  lr_scheduler_type: "linear"             # 学习率调度器类型
  
  # 日志和保存
  logging_steps: 50                       # 日志记录间隔
  save_steps: 500                         # 模型保存间隔
  eval_steps: 500                         # 验证间隔
  evaluation_strategy: "steps"            # 验证策略
  save_strategy: "steps"                  # 保存策略
  save_total_limit: 2                     # 最多保存模型数
  load_best_model_at_end: true            # 训练结束时加载最佳模型
  
  # 评估指标
  metric_for_best_model: "wer"            # 最佳模型评估指标
  greater_is_better: false                # 指标越小越好
  
  # 性能优化
  fp16: true                              # 使用FP16训练
  bf16: false                             # 使用BF16训练（A100等支持）
  gradient_checkpointing: true            # 梯度检查点（节省显存）
  dataloader_num_workers: 4               # 数据加载工作线程数
  dataloader_pin_memory: true             # 数据加载器内存锁定
  
  # 生成参数
  predict_with_generate: true             # 使用生成模式预测
  generation_max_length: 448              # 最大生成长度
  generation_num_beams: 1                 # beam search数量
  
  # 早停
  early_stopping_patience: 3              # 早停耐心值
  early_stopping_threshold: 0.001         # 早停阈值

# 监控配置
monitoring:
  use_wandb: false                        # 使用Weights & Biases
  wandb_project: "whisper-lora"           # W&B项目名称
  wandb_entity: null                      # W&B实体名称
  wandb_run_name: null                    # W&B运行名称
  
  use_tensorboard: true                   # 使用TensorBoard
  logging_dir: "./logs"                   # 日志目录
  
  report_to: ["tensorboard"]              # 报告到的平台列表

# 推理配置
inference:
  model_path: "./output"                  # 推理模型路径
  device: "auto"                          # 推理设备 (auto, cuda, cpu)
  batch_size: 8                           # 推理批次大小
  chunk_length: 30                        # 长音频分块长度（秒）
  language: null                          # 默认语言（null表示自动检测）
  task: "transcribe"                      # 默认任务类型
  return_timestamps: false                # 默认是否返回时间戳

# 系统配置
system:
  seed: 42                                # 随机种子
  local_rank: -1                          # 本地rank（分布式训练）
  deepspeed: null                         # DeepSpeed配置文件路径
  
# 数据增强（可选）
augmentation:
  enabled: false                          # 是否启用数据增强
  noise_prob: 0.1                         # 添加噪声概率
  speed_prob: 0.1                         # 变速概率
  pitch_prob: 0.1                         # 变调概率

# 模型规格建议
# whisper-tiny:    内存需求低，速度快，准确率一般，适合快速原型
# whisper-base:    平衡的选择，中等内存需求和准确率
# whisper-small:   推荐选择，较好的准确率和合理的内存需求
# whisper-medium:  高准确率，需要更多内存和计算资源
# whisper-large:   最高准确率，需要大量内存（16GB+显存）

# LoRA参数建议
# 对于小数据集：r=4-8, alpha=16-32
# 对于中等数据集：r=8-16, alpha=32-64
# 对于大数据集：r=16-32, alpha=64-128
# dropout通常设置为0.05-0.1

# 训练建议
# 1. 从小模型和小数据集开始试验
# 2. 根据显存大小调整批次大小
# 3. 使用gradient_checkpointing节省显存
# 4. 监控训练损失和验证WER
# 5. 适当调整学习率和训练轮数