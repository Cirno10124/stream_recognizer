#!/usr/bin/env python3
"""
DeepSeek-Coder 7B vLLMéƒ¨ç½²é…ç½®
æ”¯æŒç»å¯¹è·¯å¾„å’Œå¤šç§é…ç½®é€‰é¡¹
é€‚ç”¨äºä¸Whisper-Largeå…±å­˜çš„3090æœåŠ¡å™¨
"""

import os
import sys
import argparse
from pathlib import Path
from vllm import LLM, SamplingParams
from vllm.entrypoints.openai.api_server import run_server

def get_deployment_config(model_path=None):
    """è·å–éƒ¨ç½²é…ç½®"""
    # é»˜è®¤æ¨¡å‹è·¯å¾„ï¼ˆå¯ä»¥æ˜¯ç»å¯¹è·¯å¾„æˆ–HuggingFaceæ¨¡å‹åï¼‰
    default_model = model_path or "deepseek-ai/deepseek-coder-7b-instruct-v1.5"
    
    return {
        # æ¨¡å‹é…ç½®
        "model": default_model,
        "tokenizer": default_model,  # é€šå¸¸ä¸æ¨¡å‹è·¯å¾„ç›¸åŒ
        
        # ç¡¬ä»¶é…ç½®
        "tensor_parallel_size": 1,  # å•GPU
        "gpu_memory_utilization": 0.75,  # ä¸ºWhisperé¢„ç•™æ˜¾å­˜
        "max_model_len": 4096,
        "block_size": 16,
        
        # æœåŠ¡é…ç½®
        "host": "0.0.0.0",
        "port": 8000,
        "api_key": None,  # å¯é€‰ï¼šè®¾ç½®APIå¯†é’¥
        
        # æ€§èƒ½ä¼˜åŒ–
        "quantization": None,  # None, "awq", "gptq"
        "dtype": "half",  # FP16ç²¾åº¦
        "swap_space": 4,  # 4GB swap space
        
        # æ¨ç†é…ç½®
        "disable_log_stats": False,
        "max_num_seqs": 64,  # æœ€å¤§å¹¶å‘åºåˆ—
        "max_num_batched_tokens": 8192,
        "trust_remote_code": True,  # ä¿¡ä»»è¿œç¨‹ä»£ç 
    }

def validate_model_path(model_path):
    """éªŒè¯æ¨¡å‹è·¯å¾„æ˜¯å¦æœ‰æ•ˆ"""
    if model_path is None:
        return True  # ä½¿ç”¨é»˜è®¤HuggingFaceæ¨¡å‹
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºç»å¯¹è·¯å¾„
    if os.path.isabs(model_path):
        if not os.path.exists(model_path):
            print(f"âŒ é”™è¯¯ï¼šæ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
            return False
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å¿…è¦çš„æ¨¡å‹æ–‡ä»¶
        required_files = ["config.json"]
        for file in required_files:
            if not os.path.exists(os.path.join(model_path, file)):
                print(f"âš ï¸  è­¦å‘Šï¼šæ¨¡å‹ç›®å½•ä¸­ç¼ºå°‘ {file}")
        
        print(f"âœ… æ¨¡å‹è·¯å¾„éªŒè¯é€šè¿‡: {model_path}")
        return True
    else:
        # HuggingFaceæ¨¡å‹åç§°
        print(f"ğŸ“¥ å°†ä»HuggingFaceä¸‹è½½æ¨¡å‹: {model_path}")
        return True

def check_dependencies():
    """æ£€æŸ¥å¿…è¦çš„ä¾èµ–"""
    required_packages = ["vllm", "torch", "transformers"]
    missing_packages = []
    
    for package in required_packages:
        try:
            __import__(package)
            print(f"âœ… {package} å·²å®‰è£…")
        except ImportError:
            missing_packages.append(package)
            print(f"âŒ {package} æœªå®‰è£…")
    
    if missing_packages:
        print(f"\nè¯·å®‰è£…ç¼ºå¤±çš„åŒ…:")
        print(f"pip install {' '.join(missing_packages)}")
        return False
    
    return True

def deploy_with_vllm(config):
    """ä½¿ç”¨vLLMéƒ¨ç½²DeepSeekæ¨¡å‹"""
    print("ğŸš€ å¯åŠ¨DeepSeek-Coder 7Béƒ¨ç½²...")
    print(f"ğŸ“ æ¨¡å‹è·¯å¾„: {config['model']}")
    print(f"ğŸ“Š GPUå†…å­˜åˆ©ç”¨ç‡: {config['gpu_memory_utilization']*100}%")
    print(f"ğŸ”§ é‡åŒ–æ–¹æ¡ˆ: {config['quantization'] or 'æ— '}")
    print(f"ğŸŒ æœåŠ¡åœ°å€: http://{config['host']}:{config['port']}")
    print(f"ğŸ”¢ æœ€å¤§åºåˆ—é•¿åº¦: {config['max_model_len']}")
    
    # è®¾ç½®ç¯å¢ƒå˜é‡
    os.environ["CUDA_VISIBLE_DEVICES"] = "0"  # ä½¿ç”¨ç¬¬ä¸€ä¸ªGPU
    
    try:
        # æ„å»ºLLMå‚æ•°
        llm_kwargs = {
            "model": config["model"],
            "tokenizer": config["tokenizer"],
            "tensor_parallel_size": config["tensor_parallel_size"],
            "gpu_memory_utilization": config["gpu_memory_utilization"],
            "max_model_len": config["max_model_len"],
            "dtype": config["dtype"],
            "swap_space": config["swap_space"],
            "max_num_seqs": config["max_num_seqs"],
            "max_num_batched_tokens": config["max_num_batched_tokens"],
            "trust_remote_code": config["trust_remote_code"],
        }
        
        # åªåœ¨æœ‰é‡åŒ–æ—¶æ·»åŠ é‡åŒ–å‚æ•°
        if config["quantization"]:
            llm_kwargs["quantization"] = config["quantization"]
        
        print("ğŸ”„ æ­£åœ¨åŠ è½½æ¨¡å‹...")
        
        # åˆå§‹åŒ–LLM
        llm = LLM(**llm_kwargs)
        
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼")
        print("ğŸš€ å¯åŠ¨APIæœåŠ¡å™¨...")
        
        # å¯åŠ¨APIæœåŠ¡å™¨
        run_server(
            engine=llm,
            host=config["host"],
            port=config["port"],
            api_key=config["api_key"],
            disable_log_stats=config["disable_log_stats"]
        )
        
    except Exception as e:
        print(f"âŒ éƒ¨ç½²å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        raise

def main():
    parser = argparse.ArgumentParser(
        description="Deploy DeepSeek with vLLM",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  # ä½¿ç”¨HuggingFaceæ¨¡å‹
  python deepseek_vllm_config.py
  
  # ä½¿ç”¨æœ¬åœ°æ¨¡å‹è·¯å¾„
  python deepseek_vllm_config.py --model /mnt/models/deepseek-coder-7b-instruct-v1.5
  
  # è‡ªå®šä¹‰é…ç½®
  python deepseek_vllm_config.py --model /path/to/model --port 8001 --gpu-util 0.8 --quantization awq
        """
    )
    
    parser.add_argument("--model", type=str, 
                       help="æ¨¡å‹è·¯å¾„ï¼ˆç»å¯¹è·¯å¾„ï¼‰æˆ–HuggingFaceæ¨¡å‹å")
    parser.add_argument("--tokenizer", type=str, 
                       help="åˆ†è¯å™¨è·¯å¾„ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä¸æ¨¡å‹ç›¸åŒï¼‰")
    parser.add_argument("--port", type=int, default=8000, 
                       help="APIæœåŠ¡ç«¯å£ (é»˜è®¤: 8000)")
    parser.add_argument("--host", type=str, default="0.0.0.0", 
                       help="ç›‘å¬åœ°å€ (é»˜è®¤: 0.0.0.0)")
    parser.add_argument("--gpu-util", type=float, default=0.75, 
                       help="GPUå†…å­˜åˆ©ç”¨ç‡ (é»˜è®¤: 0.75)")
    parser.add_argument("--max-len", type=int, default=4096, 
                       help="æœ€å¤§åºåˆ—é•¿åº¦ (é»˜è®¤: 4096)")
    parser.add_argument("--quantization", choices=["awq", "gptq", "none"], default="none", 
                       help="é‡åŒ–æ–¹æ¡ˆ (é»˜è®¤: none)")
    parser.add_argument("--dtype", choices=["half", "float16", "bfloat16", "float32"], default="half", 
                       help="æ•°æ®ç±»å‹ (é»˜è®¤: half)")
    parser.add_argument("--max-seqs", type=int, default=64, 
                       help="æœ€å¤§å¹¶å‘åºåˆ—æ•° (é»˜è®¤: 64)")
    parser.add_argument("--check-deps", action="store_true", 
                       help="ä»…æ£€æŸ¥ä¾èµ–è€Œä¸å¯åŠ¨æœåŠ¡")
    
    args = parser.parse_args()
    
    # æ£€æŸ¥ä¾èµ–
    if not check_dependencies():
        sys.exit(1)
    
    if args.check_deps:
        print("âœ… æ‰€æœ‰ä¾èµ–æ£€æŸ¥å®Œæˆï¼")
        return
    
    # éªŒè¯æ¨¡å‹è·¯å¾„
    if not validate_model_path(args.model):
        sys.exit(1)
    
    # æ›´æ–°é…ç½®
    config = get_deployment_config(args.model)
    config["port"] = args.port
    config["host"] = args.host
    config["gpu_memory_utilization"] = args.gpu_util
    config["max_model_len"] = args.max_len
    config["max_num_seqs"] = args.max_seqs
    config["dtype"] = args.dtype
    
    # å¤„ç†é‡åŒ–é€‰é¡¹
    if args.quantization != "none":
        config["quantization"] = args.quantization
    else:
        config["quantization"] = None
    
    # å¤„ç†åˆ†è¯å™¨è·¯å¾„
    if args.tokenizer:
        config["tokenizer"] = args.tokenizer
    
    try:
        deploy_with_vllm(config)
    except KeyboardInterrupt:
        print("\nğŸ›‘ ç”¨æˆ·ä¸­æ–­ï¼Œæ­£åœ¨å…³é—­æœåŠ¡...")
    except Exception as e:
        print(f"âŒ å¯åŠ¨å¤±è´¥: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main() 